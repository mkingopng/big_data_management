# Part B - Spark RDD with CSV (4 marks)
In Part B your task is to answer a question about the data in a CSV file using Spark RDD. When you click the panel on the right 
you'll get a connection to a server that has, in your home directory, the CSV file "orders.csv". It's one that you've seen before. 
Here are the fields in the file:
```
OrderDate (date)
ISBN (string)
Title (string)
Category (string)
PriceEach (decimal)
Quantity (integer)
FirstName (string)
LastName (string)
City (string)
```

Your task is to **find the number of books ordered each day**, sorted by the number of books 
**descending**, then order date **ascending**.

Your results should appear as the following:

```
2009-04-03,10
2009-04-02,8
2009-04-01,7
2009-04-04,6
2009-03-31,5
2009-04-05,4
2009-04-08,4
```

## First (4 marks)
Write a Python program that uses Spark RDDs to do this. A file called "rdd.py" has been 
created for you - you just need to fill in the details. You should be able to modify 
programs that you have already seen in this week's content. To sort the RDD results, you 
can use SortBy, and here is an example of it. 

### Hint:

```
>>> tmp = [('a', 3), ('b', 2), ('a', 1), ('d', 4), ('2', 5)]
>>> sc.parallelize(tmp).sortBy(lambda x: (x[0],x[1])).collect()
```

### Output:
```
[('2', 5), ('a', 1), ('a', 3), ('b', 2), ('d', 4)]
```


To test your program you first need to create your default directory in Hadoop, and copy 
orders.csv to it:

`hdfs dfs -mkdir -p /user/user`

`hdfs dfs -put orders.csv`

You can test your program by running the following command:
`spark-submit rdd.py`

Please save your results in the '**result-rdd**' folder in HDFS.
